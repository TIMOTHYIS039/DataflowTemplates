{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TIMOTHYIS039/DataflowTemplates/blob/master/Langchain_Framework_OpenAI_GPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Link - https://colab.research.google.com/drive/1GxJEJwfgDpEMERi1GT2OFFMtxlHNfvZD?usp=sharing"
      ],
      "metadata": {
        "id": "j6S-cJ46GQ89"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aAOZK0Eog-d"
      },
      "source": [
        "## Install Necessary Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rb4KyIDboWzE",
        "outputId": "ecaac177-5e43-47d5-d654-57ce7b3b7c54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain==0.0.164\n",
            "  Downloading langchain-0.0.164-py3-none-any.whl (788 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m789.0/789.0 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai\n",
            "  Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken\n",
            "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-search-results\n",
            "  Downloading google_search_results-2.4.2.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.164) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.164) (2.0.20)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.164) (3.8.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.164) (4.0.3)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain==0.0.164)\n",
            "  Downloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.164) (2.8.5)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.164) (1.23.5)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain==0.0.164)\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic<2,>=1 (from langchain==0.0.164)\n",
            "  Downloading pydantic-1.10.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.164) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.164) (8.2.3)\n",
            "Requirement already satisfied: tqdm>=4.48.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.164) (4.66.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.164) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.164) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.164) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.164) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.164) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.164) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.164)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.164)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain==0.0.164) (4.7.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.164) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.164) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.164) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.164) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.164) (23.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.164)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Building wheels for collected packages: google-search-results\n",
            "  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-search-results: filename=google_search_results-2.4.2-py3-none-any.whl size=32003 sha256=2005c8898f4e58dda44ef08bfc4e3e83cf2000bf40b80f571a207334b1da20e8\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/b2/c3/03302d12bb44a2cdff3c9371f31b72c0c4e84b8d2285eeac53\n",
            "Successfully built google-search-results\n",
            "Installing collected packages: pydantic, mypy-extensions, marshmallow, typing-inspect, tiktoken, openapi-schema-pydantic, google-search-results, openai, dataclasses-json, langchain\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.2.1\n",
            "    Uninstalling pydantic-2.2.1:\n",
            "      Successfully uninstalled pydantic-2.2.1\n",
            "Successfully installed dataclasses-json-0.5.14 google-search-results-2.4.2 langchain-0.0.164 marshmallow-3.20.1 mypy-extensions-1.0.0 openai-0.28.0 openapi-schema-pydantic-1.2.4 pydantic-1.10.12 tiktoken-0.4.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain==0.0.164 openai tiktoken google-search-results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsIeQFOFoqwy"
      },
      "source": [
        "## Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLjx4eB3opiX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain.llms import OpenAI,AzureOpenAI #if you are using Azure key, then use AzureOpenAI Class\n",
        "from langchain.chat_models import ChatOpenAI,AzureChatOpenAI\n",
        "from langchain import PromptTemplate, LLMChain, FewShotPromptTemplate\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.chains import SimpleSequentialChain\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    AIMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.schema import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage\n",
        ")\n",
        "from langchain.agents import load_tools\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.agents import AgentType"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AH5HSuoptCM"
      },
      "source": [
        "## Load API Keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6a7DNkI8ptnF"
      },
      "outputs": [],
      "source": [
        "os.environ[\"OPENAI_API_TYPE\"] = 'azure'\n",
        "os.environ[\"OPENAI_API_VERSION\"] = \"2023-03-15-preview\"\n",
        "os.environ[\"OPENAI_API_BASE\"] = 'https://sandbox-ey.openai.azure.com/'\n",
        "os.environ[\"OPENAI_API_KEY\"] = '4699cbb6884a438095ba926bc0a8e12d'\n",
        "os.environ['SERPAPI_API_KEY'] = \"18e890ed0535f75ac185905d99dac7f4513907443f050acf527fa94b01f737a2\"\n",
        "davinci_model = 'text-davinci-003'\n",
        "embedding_model = 'text-embedding-ada-002'\n",
        "chat_model = 'gpt-35-turbo'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_EBtwtVu65t"
      },
      "source": [
        "## Prompts In Langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tOusdFVvJjk"
      },
      "source": [
        "**Prompts**\n",
        "\n",
        "Language models take text as input - that text is commonly referred to as a prompt.\n",
        "\n",
        "**Prompt Template**\n",
        "\n",
        "A prompt template refers to a reproducible way to generate a prompt. It contains a text string (“the template”), that can take in a set of parameters from the end user and generate a prompt.\n",
        "\n",
        "The prompt template may contain:\n",
        "\n",
        "* instructions to the language model,\n",
        "* a set of few shot examples to help the language model generate a better response,\n",
        "* a question to the language model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ccWpHEmw8by"
      },
      "source": [
        "#### Single Input Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_qYBSJLu8NS"
      },
      "outputs": [],
      "source": [
        "tagline_prompt = \"\"\"Your task is generate some catchy and appealing tagline for company, delimited by triple `.\n",
        "You should act as a marketing ads agent.\n",
        "```{company}```\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Mq4wW8ervNiQ",
        "outputId": "664dad13-cfad-49bb-ee6a-3d35a10c5522"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Your task is generate some catchy and appealing tagline for company, delimited by triple `.\\nYou should act as a marketing ads agent.\\n```Ice-cream Shop```'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "#Class to make template for prompt.\n",
        "tagline_prompt_template = PromptTemplate(\n",
        "    input_variables=[\"company\"], #Can pass any input variables dynamically\n",
        "    template=tagline_prompt,\n",
        ")\n",
        "tagline_prompt_template.format(company=\"Ice-cream Shop\") #can see how prompt will look like"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GgjyRblDXmCX",
        "outputId": "5ab86233-ee46-449c-a146-8e8631f4bcb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your task is generate some catchy and appealing tagline for company, delimited by triple `.\n",
            "You should act as a marketing ads agent.\n",
            "```Book Shop```\n"
          ]
        }
      ],
      "source": [
        "print(tagline_prompt_template.format(company=\"Book Shop\")) #can see how prompt will look like"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uV91oXadxINI"
      },
      "source": [
        "#### Multiple Input Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0ZTARFSxNE0"
      },
      "outputs": [],
      "source": [
        "qa_prompt = \"\"\"Your task is to answer the asked question, delimited by triple `, from the context, delimited by #. Return the response in json format\n",
        "Question - ```{question}```\n",
        "Context - #{context}#\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "Uemz_zSgxMxn",
        "outputId": "0b26775d-8b40-46b7-ca60-56f44cf3afc6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Your task is to answer the asked question, delimited by triple `, from the context, delimited by #. Return the response in json format\\nQuestion - ```What are some common mental health issues that can be caused by a lack of sleep?```\\nContext - #According to a recent study published in a leading medical journal, lack of sleep can have a significant impact on mental health. The study found that people who regularly get less than 7 hours of sleep per night are more likely to experience depression, anxiety, and other mental health problems. Additionally, the study found that poor sleep quality can also contribute to these issues. The authors of the study recommend that individuals prioritize getting enough sleep as part of their overall self-care routine.#'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "#Class to make template for prompt.\n",
        "qa_prompt_template = PromptTemplate(\n",
        "    input_variables=[\"question\",\"context\"], #Can pass any input variables dynamically\n",
        "    template=qa_prompt,\n",
        ")\n",
        "context = \"\"\"According to a recent study published in a leading medical journal, lack of sleep can have a significant impact on mental health. The study found that people who regularly get less than 7 hours of sleep per night are more likely to experience depression, anxiety, and other mental health problems. Additionally, the study found that poor sleep quality can also contribute to these issues. The authors of the study recommend that individuals prioritize getting enough sleep as part of their overall self-care routine.\"\"\"\n",
        "question = \"What are some common mental health issues that can be caused by a lack of sleep?\"\n",
        "qa_prompt_template.format(question=question,context=context) #can see how prompt will look like"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9zavFu9yJyd"
      },
      "source": [
        "#### Serialize your prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8axNwQkAxMse"
      },
      "outputs": [],
      "source": [
        "#tagline_prompt_template.save('tagline_prompt.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sA_B6utvyRhv"
      },
      "source": [
        "#### Pass few shot examples to a prompt template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPTKKuRcxMn0"
      },
      "outputs": [],
      "source": [
        "few_shots = [\n",
        "    {\"review\": \"This is awesome!\", \"class_\": \"+\"},\n",
        "    {\"review\": \"This is bad!\", \"class_\": \"-\"},\n",
        "    {\"review\": \"Wow that movie was amazing\", \"class_\": \"+\"},\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILCYRGpzxMj0"
      },
      "outputs": [],
      "source": [
        "fewshots_prompt = \"\"\"\n",
        "Review: {review}\n",
        "Class: {class_}\\n\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECMfK_oDzrxa"
      },
      "outputs": [],
      "source": [
        "fewshots_prompt_template = PromptTemplate(\n",
        "    input_variables=[\"review\", \"class_\"],\n",
        "    template=fewshots_prompt,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZNdI4rmuYzZ1",
        "outputId": "37dbd942-3b7c-4bb6-8dd3-23b7e83065a5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nReview: This is awesome\\nClass: +\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "fewshots_prompt_template.format(review='This is awesome',class_='+')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oL9NYbPEzrpR"
      },
      "outputs": [],
      "source": [
        "few_shot_prompt = FewShotPromptTemplate(\n",
        "    # These are the examples we want to insert into the prompt.\n",
        "    examples=few_shots,\n",
        "    # This is how we want to format the examples when we insert them into the prompt.\n",
        "    example_prompt=fewshots_prompt_template,\n",
        "    # The prefix is some text that goes before the examples in the prompt.\n",
        "    # Usually, this consists of intructions.\n",
        "    prefix=\"Assign Class to given Review\",\n",
        "    # The suffix is some text that goes after the examples in the prompt.\n",
        "    # Usually, this is where the user input will go\n",
        "    suffix=\"Review: {review}\\nClass:\",\n",
        "    # The input variables are the variables that the overall prompt expects.\n",
        "    input_variables=[\"review\"],\n",
        "    # The example_separator is the string we will use to join the prefix, examples, and suffix together with.\n",
        "    example_separator=\"\\n\\n\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vP8Gqkf-0ej_",
        "outputId": "0d7e0a8d-7714-4eea-92f1-a3e2c4fd16dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assign Class to given Review\n",
            "\n",
            "\n",
            "Review: This is awesome!\n",
            "Class: +\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Review: This is bad!\n",
            "Class: -\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Review: Wow that movie was amazing\n",
            "Class: +\n",
            "\n",
            "\n",
            "\n",
            "Review: The movie was damn awesome\n",
            "Class:\n"
          ]
        }
      ],
      "source": [
        "print(few_shot_prompt.format(review=\"The movie was damn awesome\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMeCnpfdpAYj"
      },
      "source": [
        "## Access Models in Langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWOLDf7ApmqF"
      },
      "source": [
        "* **LLMs**\n",
        "\n",
        "Large Language Models (LLMs) are the first type of models we cover. These models take a text string as input, and return a text string as output.\n",
        "\n",
        "* **Chat Models**\n",
        "\n",
        "Chat Models are the second type of models we cover. These models are usually backed by a language model, but their APIs are more structured. Specifically, these models take a list of Chat Messages as input, and return a Chat Message.\n",
        "\n",
        "There are 3 main roles:\n",
        "* User => end user, you\n",
        "* Assistant => ChatGPT, model\n",
        "* System => Acts as a whisperer to assistant’s ear and guiding its responses without the user being aware of it\n",
        "\n",
        "* **Text Embedding Models**\n",
        "\n",
        "The third type of models we cover are text embedding models. These models take text as input and return a list of floats."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWuAjzH9pYtI"
      },
      "source": [
        "### LLM in Langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zwlUQfHpCQ-"
      },
      "outputs": [],
      "source": [
        "llm = AzureOpenAI(deployment_name=davinci_model,model_name=\"text-davinci-003\", n=2, best_of=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_yOAKd1rPIg"
      },
      "source": [
        "#### Make a single query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "nmbCG-Zhq-zT",
        "outputId": "fea6dcc8-dca3-458e-c199-fe1f9e851c71"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\nProgramming with Python is an easy ride\\nIt's powerful and yet simple to apply\\nIt's really nice for coding and debugging\\nA fun language to be exploring\\n\\nWe can use it for automation\\nData analysis and visualization\\nIt's also great for scientific computing\\nNo need for tedious computing\\n\\nPython's great for web development\\nAnd scripting tasks, just a breeze\\nIt supports object-oriented code\\nFor applications, it's the key\\n\\nWith Python, we can build websites\\nAnd create powerful algorithms\\nIt's an invaluable language\\nFor software development, it's the norm\\n\\nPython is an amazing language\\nIt's here to stay, no doubt\\nIts remarkable features will keep us\\nIn programming, we won't be left out\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "llm(\"write a poem on python programming?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VDCwVuYrb2K"
      },
      "source": [
        "#### Make multiple inputs to the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJmjl3HeraLM"
      },
      "outputs": [],
      "source": [
        "output = llm.generate([\"write a poem on python programming?\",\"What is the capital of India?\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cec6NXEkrr7w",
        "outputId": "a9f109aa-d93e-4858-fc06-d725cd193ad5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[Generation(text=\"\\n\\nProgramming in Python is a grand old game \\nIt's useful and helpful, though sometimes a pain \\nIt can be daunting, but keep up the pace \\nAnd you'll soon be an expert in this grand language of grace \\n\\nIt'll take time and practice, no doubt about that \\nBut soon you'll be writing code that's top-notch and fat \\nHere's to mastering Python, and pushing the limit \\nIt's a language that's helpful, no matter how you spin it \\n\\nFrom web development to data science, Python's the way\\nIt's a powerful language, here to stay \\nSo keep practicing, and you'll be just fine \\nPython's here to stay, so make it your lifeline!\", generation_info={'finish_reason': 'stop', 'logprobs': None}),\n",
              "  Generation(text=\"\\n\\nPython programming is a great way\\nTo learn the basics of coding each day\\nIt's efficient, intuitive, and powerful too\\nSo many things you can do\\n\\nStart with the basics of syntax and loops\\nThen move onto working with objects and groups\\nOnce you've mastered the fundamentals\\nYou'll be ready to tackle any challenge that comes\\n\\nFunctions, modules, classes, and imports\\nWill help you build applications of all sorts\\nFrom web development to data science\\nYou'll be able to program with confidence\\n\\nPython programming has something for everyone\\nTo learn and explore, it's quite fun\\nIt's a great way to jumpstart your career\\nAnd become a master coder this year!\", generation_info={'finish_reason': 'stop', 'logprobs': None})],\n",
              " [Generation(text='\\n\\nThe capital of India is New Delhi.', generation_info={'finish_reason': 'stop', 'logprobs': None}),\n",
              "  Generation(text='\\n\\nThe capital of India is New Delhi.', generation_info={'finish_reason': 'stop', 'logprobs': None})]]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "output.generations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Go9LaYg_sC-I"
      },
      "source": [
        "#### Check the number of tokens in Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmCLVDg2r677",
        "outputId": "97f53e0f-2296-4ac9-e1e7-f47920fd4dd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of words and tokens - 144 and 227\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"We present IMAGEBIND, an approach to learn a joint embedding across six different modalities - images, text, audio, depth, thermal, and IMU data. We show that all combinations of paired data are not necessary to train such a joint\n",
        "embedding, and only image-paired data is sufficient to bind the modalities together. IMAGEBIND can leverage recent large scale vision-language models, and extends their zeroshot capabilities to new modalities just by using their natural pairing with images. It enables novel emergent applications ‘out-of-the-box’ including cross-modal retrieval, composing modalities with arithmetic, cross-modal detection\n",
        "and generation. The emergent capabilities improve with the strength of the image encoder and we set a new state-of-theart on emergent zero-shot recognition tasks across modalities, outperforming specialist supervised models. Finally,\n",
        "we show strong few-shot recognition results outperforming prior work, and that IMAGEBIND serves as a new way to evaluate vision models for visual and non-visual tasks.\"\"\"\n",
        "words_len = len(prompt.split(' '))\n",
        "token_len = llm.get_num_tokens(prompt)\n",
        "print(f'Number of words and tokens - {words_len} and {token_len}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCTzvrH-s0zi"
      },
      "source": [
        "### ChatModel in Langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMaPal_TsQiB"
      },
      "outputs": [],
      "source": [
        "chat = AzureChatOpenAI(temperature = 0,deployment_name=chat_model,model_name=chat_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ic4Op_XF2Nl4"
      },
      "source": [
        "#### Basic Prompt for Chat Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3hNrmT_uwhS",
        "outputId": "d3780d5c-5f31-42af-c3c6-565dd2c38057"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"J'adore la programmation.\", additional_kwargs={}, example=False)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
        "    HumanMessage(content=\"I love programming.\")\n",
        "]\n",
        "chat(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqCDIv-v2pcu"
      },
      "source": [
        "#### Prompt with Instructions for ChatModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJuaCfPx1x8W"
      },
      "outputs": [],
      "source": [
        "template=\"You are a helpful assistant that translates english to pirate.\"\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
        "example_human = HumanMessagePromptTemplate.from_template(\"Hi\")\n",
        "example_ai = AIMessagePromptTemplate.from_template(\"Argh me mateys\")\n",
        "human_template=\"{text}\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVt24yDm6d35"
      },
      "outputs": [],
      "source": [
        "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, example_human, example_ai, human_message_prompt])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNAfw_SV6uq0",
        "outputId": "53c908c4-ab1f-436b-ec9f-9cb70f68e720"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "System: You are a helpful assistant that translates english to pirate.\n",
            "Human: Hi\n",
            "AI: Argh me mateys\n",
            "Human: I love programming.\n"
          ]
        }
      ],
      "source": [
        "print(chat_prompt.format(text=\"I love programming.\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4Zl-QEe8Uh0"
      },
      "source": [
        "### Embedding Model in Langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIyD0R-W8UDE"
      },
      "outputs": [],
      "source": [
        "embeddings = OpenAIEmbeddings(deployment=embedding_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJl18RXe7m__"
      },
      "outputs": [],
      "source": [
        "sentence = \"I need to craete embedding of this sentece\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSpeMZnq806D",
        "outputId": "cd95297b-d2b6-4281-cd45-2c06076b6e21"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1536"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "embd = embeddings.embed_query(sentence)\n",
        "len(embd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l99XBnen9BDh"
      },
      "source": [
        "## Chains in Langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kRyV0s2-HDh"
      },
      "source": [
        "Using an LLM in isolation is fine for some simple applications, but many more complex ones require chaining LLMs - either with each other or with other experts. LangChain provides a standard interface for Chains, as well as some common implementations of chains for ease of use.\n",
        "\n",
        "Chains are what you get by connecting one or more large language models (LLMs) in a logical way. (Chains can be built of entities other than LLMs but for now, let’s stick with this definition for simplicity)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VizThCFBnlnv"
      },
      "source": [
        "#### Run a chain in Langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YKD_wpGoWBX"
      },
      "source": [
        "##### Using Non-Chat Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrWhrCKB9Dkp"
      },
      "outputs": [],
      "source": [
        "prompt_tagline = PromptTemplate(\n",
        "    input_variables=[\"product\"],\n",
        "    template=\"Write a catchy and appealing tagline for {product}?\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwVlEiHhoAMr"
      },
      "outputs": [],
      "source": [
        "chain = LLMChain(llm=llm, prompt=prompt_tagline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgrGKBPGoCQh",
        "outputId": "f26f7471-e22c-426b-fbed-00549a6c654f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "print(chain.run(\"Ice-cream shop\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6y1K7rTvcL53",
        "outputId": "1561de81-4198-4d35-9b31-9eeac73b2eaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\"Discover Your Next Adventure - Shop Books Now!\"\n"
          ]
        }
      ],
      "source": [
        "print(chain.run(\"Book shop\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1i1WsJHoSP7"
      },
      "source": [
        "##### Using Chat Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdGHS2mkoPa4"
      },
      "outputs": [],
      "source": [
        "human_message_prompt = HumanMessagePromptTemplate(\n",
        "        prompt=PromptTemplate(\n",
        "            template=\"Write a catchy and appealing tagline for {product}?\",\n",
        "            input_variables=[\"product\"],\n",
        "        )\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8ketmfeon4C"
      },
      "outputs": [],
      "source": [
        "chat_prompt_template = ChatPromptTemplate.from_messages([human_message_prompt])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xdtz-LCFoqMh"
      },
      "outputs": [],
      "source": [
        "chat_chain = LLMChain(llm=chat, prompt=chat_prompt_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5rKk1sxyotlW",
        "outputId": "f894d814-02c1-450c-933d-37b5f9fc3a4f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\"Scoops of happiness in every cone!\"'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "chat_chain.run({'product':\"Ice-cream shop\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yi9oqEfio3Jx"
      },
      "source": [
        "#### Multiple values in a single Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4v_hlIroxyv"
      },
      "outputs": [],
      "source": [
        "qa_prompt = \"\"\"Your task is to answer the asked question, delimited by triple `, from the context, delimited by #. Return the response in json format\n",
        "Question - ```{question}```\n",
        "Context - #{context}#\"\"\"\n",
        "#Class to make template for prompt.\n",
        "qa_prompt_template = PromptTemplate(\n",
        "    input_variables=[\"question\",\"context\"], #Can pass any input variables dynamically\n",
        "    template=qa_prompt,\n",
        ")\n",
        "context = \"\"\"According to a recent study published in a leading medical journal, lack of sleep can have a significant impact on mental health. The study found that people who regularly get less than 7 hours of sleep per night are more likely to experience depression, anxiety, and other mental health problems. Additionally, the study found that poor sleep quality can also contribute to these issues. The authors of the study recommend that individuals prioritize getting enough sleep as part of their overall self-care routine.\"\"\"\n",
        "question = \"What are some common mental health issues that can be caused by a lack of sleep?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4iLTCfGphdW"
      },
      "outputs": [],
      "source": [
        "chain = LLMChain(llm=llm, prompt=qa_prompt_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOvs_oXmpmyx",
        "outputId": "b8dd2d33-795b-4f91-b671-050517465410"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "{\n",
            "  \"answer\": \"The study found that people who regularly get less than 7 hours of sleep per night are more likely to experience depression, anxiety, and other mental health problems. Additionally, the study found that poor sleep quality can also contribute to these issues.\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "print(chain.run({'question':question,'context':context}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AT-yS-tRp9ZH"
      },
      "source": [
        "#### Debug a Chain (verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkrE2YNzpsU7"
      },
      "outputs": [],
      "source": [
        "debug_chain = LLMChain(llm=llm, prompt=qa_prompt_template,verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKdBf7MTqNrv",
        "outputId": "fca0603f-b755-4f4c-afe8-6c30013374c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYour task is to answer the asked question, delimited by triple `, from the context, delimited by #. Return the response in json format\n",
            "Question - ```What are some common mental health issues that can be caused by a lack of sleep?```\n",
            "Context - #According to a recent study published in a leading medical journal, lack of sleep can have a significant impact on mental health. The study found that people who regularly get less than 7 hours of sleep per night are more likely to experience depression, anxiety, and other mental health problems. Additionally, the study found that poor sleep quality can also contribute to these issues. The authors of the study recommend that individuals prioritize getting enough sleep as part of their overall self-care routine.#\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "{\n",
            "  \"response\": \"Depression, anxiety, and other mental health problems are some of the most common mental health issues that can be caused by a lack of sleep.\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "print(debug_chain.run({'question':question,'context':context}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmVCnkC2qTsA"
      },
      "source": [
        "#### Running Series of Chains"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQCsoSKDqQbD"
      },
      "outputs": [],
      "source": [
        "# first prompt to get the company name\n",
        "first_prompt = prompt = PromptTemplate(\n",
        "    input_variables=[\"product\"],\n",
        "    template=\"What is a good name for a company that makes {product}?\",\n",
        ")\n",
        "\n",
        "# second prompt to get catchphrase for the company name\n",
        "second_prompt = PromptTemplate(\n",
        "    input_variables=[\"company_name\"],\n",
        "    template=\"Write a tagline for the following company: {company_name}\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvIU-wxArnMv"
      },
      "outputs": [],
      "source": [
        "first_chain = LLMChain(llm=llm, prompt=first_prompt)\n",
        "second_chain = LLMChain(llm=llm, prompt=second_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZ9EuAzXrzSZ"
      },
      "outputs": [],
      "source": [
        "complete_chain = SimpleSequentialChain(chains=[first_chain, second_chain], verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FBFSoaBsBHu",
        "outputId": "3d4fb66e-5636-47c8-b276-073989bceee5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3m\n",
            "\n",
            "Pizza Paradise.\u001b[0m\n",
            "\u001b[33;1m\u001b[1;3m\n",
            "\n",
            "\"A Slice of Heaven in Every Bite!\"\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\"A Slice of Heaven in Every Bite!\"\n"
          ]
        }
      ],
      "source": [
        "catchphrase = complete_chain.run(\"Delicious Pizza\")\n",
        "print(catchphrase)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxWPSeGb7w_3"
      },
      "source": [
        "## Agent in Langchain (Optional)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQOXwjS478IO"
      },
      "source": [
        "**Tools**\n",
        "\n",
        "Tools are ways that an agent can use to interact with the outside world.\n",
        "\n",
        "**Agents**\n",
        "\n",
        "Some applications will require not just a predetermined chain of calls to LLMs/other tools, but potentially an unknown chain that depends on the user’s input. In these types of chains, there is a “agent” which has access to a suite of tools. Depending on the user input, the agent can then decide which, if any, of these tools to call.\n",
        "\n",
        "**Agent Executor**\n",
        "\n",
        "Agent executors take an agent and tools and use the agent to decide which tools to call and in what order.\n",
        "\n",
        "In this part of the documentation we cover other related functionality to agent executors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8V-nHViATdz"
      },
      "source": [
        "### Loading Tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rZaQjLHsIK8"
      },
      "outputs": [],
      "source": [
        "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R50mxDa--Aos"
      },
      "outputs": [],
      "source": [
        "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "4aUAIOe4_jfz",
        "outputId": "f34e68c8-e46a-45ff-a550-20e94cb80042"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I need to access information about the stock market\n",
            "Action: Search\n",
            "Action Input: \"Apple Market Capital\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mApple Inc. is an American multinational technology company headquartered in Cupertino, California. Apple is the world's largest technology company by revenue, with US$394.3 billion in 2022 revenue. As of March 2023, Apple is the world's biggest company by market capitalization.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
            "Final Answer: As of March 2023, Apple's market capitalization was US$394.3 billion.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"As of March 2023, Apple's market capitalization was US$394.3 billion.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "agent.run(\"what is the market capital of Apple?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfKjsy3qAI4e"
      },
      "source": [
        "### Calling using LLM only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "skc5lds1_1mi",
        "outputId": "a9d3c8bd-3f57-4989-9b65-b4ae266e62a1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n2^.43 = 1.5229'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "llm(\"what is 2 raised to the power .43?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4a4pZ-uANq4"
      },
      "source": [
        "### Calling using LLM Calculation Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "H3Zyh3FT_nbk",
        "outputId": "33d8a66d-71ed-4213-c8c8-67bee9fc1a34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I need to calculate this\n",
            "Action: Calculator\n",
            "Action Input: 2^.43\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mAnswer: 1.3472335768656902\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
            "Final Answer: 2^.43 = 1.3472335768656902\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2^.43 = 1.3472335768656902'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "agent.run(\"what is 2 raised to the power .43?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DF4HZ2f0_sud"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "SsIeQFOFoqwy",
        "B_EBtwtVu65t",
        "MMeCnpfdpAYj",
        "O8V-nHViATdz",
        "ZfKjsy3qAI4e",
        "C4a4pZ-uANq4"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}